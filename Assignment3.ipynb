{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AithaDhanush/NLP/blob/main/Assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AITHA DHANUSH-20K45A0228\n",
        "\n",
        "NLP ASSIGNMENT - 3\n",
        "\n",
        "A paragraph is a series of sentences that are organized and coherent, and are all related to a single topic. Almost every piece of writing you do that is longer than a few sentences should be organized into paragraphs. This is because paragraphs show a reader where the subdivisions of an essay begin and end, and thus help the reader see the organization of the essay and grasp its main points.\n",
        "\n",
        "Paragraphs can contain many different kinds of information. A paragraph could contain a series of brief examples or a single long illustration of a general point. It might describe a place, character, or process; narrate a series of events; compare or contrast two or more things; classify items into categories; or describe causes and effects. Regardless of the kind of information they contain, all paragraphs share certain characteristics. One of the most important of these is a topic sentence.\n",
        "\n",
        "QUESTIONS:\n",
        "\n",
        "1.Convert the above paragraph into vectors using:\n",
        "\n",
        "  i) Word2vec\n",
        "\n",
        "  ii) USE\n",
        "\n",
        "  iii) ELMO\n",
        "\n",
        "  iv) GP2\n",
        "\n",
        "  v) Sentence-BERT\n",
        "\n",
        "2.Find named entities (NER) for the above paragraph?\n",
        "\n",
        "3.Find similar sentences (repeated sentences) from the above paragraph? (Cosine Similarity, use BERT to encode)\n",
        "\n",
        "4.Explain POS tagging with HMM? Tag POS for the above paragraph?\n"
      ],
      "metadata": {
        "id": "4I0mOEIlhEyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assigning the given paragraph to a variable\n",
        "\n",
        "para = '''A paragraph is a series of sentences that are organized and coherent, and are all related to a single topic. Almost every piece of writing you do that is longer than a few sentences should be organized into paragraphs. This is because paragraphs show a reader where the subdivisions of an essay begin and end, and thus help the reader see the organization of the essay and grasp its main points.\n",
        "Paragraphs can contain many different kinds of information. A paragraph could contain a series of brief examples or a single long illustration of a general point. It might describe a place, character, or process; narrate a series of events; compare or contrast two or more things; classify items into categories; or describe causes and effects. Regardless of the kind of information they contain, all paragraphs share certain characteristics. One of the most important of these is a topic sentence.\n",
        "'''\n",
        "para"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "bt9bh-Lua6xT",
        "outputId": "2e4f8487-5455-45f9-8a82-72970fda80f5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A paragraph is a series of sentences that are organized and coherent, and are all related to a single topic. Almost every piece of writing you do that is longer than a few sentences should be organized into paragraphs. This is because paragraphs show a reader where the subdivisions of an essay begin and end, and thus help the reader see the organization of the essay and grasp its main points.\\nParagraphs can contain many different kinds of information. A paragraph could contain a series of brief examples or a single long illustration of a general point. It might describe a place, character, or process; narrate a series of events; compare or contrast two or more things; classify items into categories; or describe causes and effects. Regardless of the kind of information they contain, all paragraphs share certain characteristics. One of the most important of these is a topic sentence.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "            1.Converting the paragraph to vectors:\n"
      ],
      "metadata": {
        "id": "jJjYMoi0kvN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing required packages\n",
        "\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import gensim\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import word2vec\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wj40YPPbHCJ",
        "outputId": "15c2ecfa-9574-422e-832c-8418344482ca"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#code to convert paragraph to sentences\n",
        "\n",
        "def essay_to_sentences(paragraph):\n",
        "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "    raw_sentences = tokenizer.tokenize(paragraph.strip())\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        if len(raw_sentence) > 0:\n",
        "            sentences.append((raw_sentence))\n",
        "    return sentences\n",
        "\n",
        "sentences=essay_to_sentences(para)\n",
        "\n",
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhyU6RwobN3t",
        "outputId": "84af9724-74e5-4a1e-95ce-6d16bdd5e10a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A paragraph is a series of sentences that are organized and coherent, and are all related to a single topic.',\n",
              " 'Almost every piece of writing you do that is longer than a few sentences should be organized into paragraphs.',\n",
              " 'This is because paragraphs show a reader where the subdivisions of an essay begin and end, and thus help the reader see the organization of the essay and grasp its main points.',\n",
              " 'Paragraphs can contain many different kinds of information.',\n",
              " 'A paragraph could contain a series of brief examples or a single long illustration of a general point.',\n",
              " 'It might describe a place, character, or process; narrate a series of events; compare or contrast two or more things; classify items into categories; or describe causes and effects.',\n",
              " 'Regardless of the kind of information they contain, all paragraphs share certain characteristics.',\n",
              " 'One of the most important of these is a topic sentence.']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "               i) Word2vec"
      ],
      "metadata": {
        "id": "sYBVjsrIklsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing the word2vec model, converting words to vectors and removing stop words\n",
        "\n",
        "wordvecs=[nltk.word_tokenize(s) for s in sentences]\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stops_words=list(set(stopwords.words(\"english\")))\n",
        "\n",
        "for i in wordvecs:\n",
        "  for j in i:\n",
        "    if j in stops_words:\n",
        "      i.remove(j)\n",
        "    elif len(j)==1:\n",
        "      i.remove(j)\n",
        "\n",
        "model=gensim.models.Word2Vec(wordvecs,min_count=1)\n",
        "\n",
        "#printing the converted vector form of word 'organized'\n",
        "\n",
        "model.wv['organized']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2I-Z__-bawR",
        "outputId": "6228d1d7-fb9d-40ea-e5c4-9742284569ce"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2.1974577e-03,  8.7372359e-04,  5.8032334e-04,  6.8503039e-05,\n",
              "       -2.0794470e-03, -2.8860616e-03, -4.7948025e-03,  4.4388636e-03,\n",
              "       -1.0703031e-03, -1.1377762e-03,  9.5640519e-04, -8.6082780e-04,\n",
              "        3.7946823e-04,  3.0437857e-03, -9.4066310e-04, -3.3182860e-04,\n",
              "       -8.4300886e-04, -3.9148089e-03, -9.4656210e-04,  4.9699144e-03,\n",
              "       -1.4494389e-03,  4.9066455e-03,  5.0659588e-04,  1.5139730e-03,\n",
              "        4.6348898e-03, -3.8106423e-03,  3.1651203e-03, -4.1498025e-03,\n",
              "       -4.7920924e-03,  3.6873263e-03, -2.5480700e-04,  1.8789023e-03,\n",
              "       -9.5875614e-04,  2.3569630e-03, -4.8609157e-03, -1.8465371e-03,\n",
              "        5.8632670e-04,  7.5559312e-04, -5.8899820e-04, -1.2025842e-03,\n",
              "       -1.4439881e-04, -1.4381881e-03, -4.7648554e-03, -1.0047412e-03,\n",
              "       -2.8806771e-03,  4.0394505e-03,  3.4253276e-04, -1.7963352e-03,\n",
              "        4.5555858e-03, -3.3064631e-03,  1.8532930e-03,  4.8454846e-03,\n",
              "       -7.9373736e-04, -4.7833440e-03, -3.6257752e-03, -4.1972357e-03,\n",
              "       -4.0569901e-03, -7.4000628e-04, -4.6591377e-03, -5.5049361e-05,\n",
              "       -1.2017596e-03, -1.9084424e-04,  1.3381463e-03,  1.3625732e-03,\n",
              "       -4.7231875e-03,  2.9600775e-03, -3.3336114e-03,  4.2041275e-03,\n",
              "        4.6195090e-03,  3.8748628e-03, -7.8607001e-04,  3.4673687e-03,\n",
              "       -4.5981249e-03,  3.1964417e-04,  3.6172569e-03,  3.9163292e-03,\n",
              "        4.8264726e-03,  3.1467373e-03, -8.9626375e-04,  2.7163927e-03,\n",
              "        8.3124510e-04,  2.5478220e-03,  1.2376537e-03,  3.5574827e-03,\n",
              "       -2.9403339e-03,  4.9677347e-03, -4.4767451e-04, -1.8965368e-03,\n",
              "       -4.0017823e-03,  2.6642398e-03,  1.6042701e-03, -8.4218703e-04,\n",
              "        1.8777779e-03,  3.8503616e-03, -7.9154037e-04, -1.3489382e-03,\n",
              "        3.6059688e-03,  1.2743325e-03,  2.0244007e-03,  7.0491899e-04],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "              ii) USE \n"
      ],
      "metadata": {
        "id": "66WlH6gOkcxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the USE Model\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "vect=hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "#converting to vectors\n",
        "res_vectors=vect(sentences)\n",
        "print(res_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61MQYDzabttj",
        "outputId": "a9956c7b-a2ed-452f-b48c-9c574a9c8807"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[ 0.01168496 -0.03060572  0.06116336 ... -0.08641755  0.0002505\n",
            "   0.05482749]\n",
            " [ 0.02972507 -0.03655469  0.08002593 ... -0.07038905 -0.0283213\n",
            "   0.04804676]\n",
            " [ 0.07221661 -0.04182237  0.05336686 ... -0.06942353  0.01795934\n",
            "   0.06641504]\n",
            " ...\n",
            " [ 0.01586947 -0.05243036  0.0606509  ... -0.06435591  0.04215747\n",
            "   0.06304204]\n",
            " [ 0.04141247  0.02588907 -0.0062563  ... -0.02162989  0.00910817\n",
            "   0.03623575]\n",
            " [ 0.01578411 -0.02142678  0.00402448 ... -0.09605585 -0.06707881\n",
            "   0.07970381]], shape=(8, 512), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"shape= \",res_vectors[0].shape)\n",
        "\n",
        "#each sentence is converted into vector having 512 values\n",
        "\n",
        "print(\"The sentence: \",sentences[0],\"\\n is converted as : \\n{}\".format(res_vectors[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQMapdqUcNHR",
        "outputId": "9eeaf0a8-fc2f-4561-a34e-5c57577c3b6d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape=  (512,)\n",
            "The sentence:  A paragraph is a series of sentences that are organized and coherent, and are all related to a single topic. \n",
            " is converted as : \n",
            "[ 1.16849625e-02 -3.06057241e-02  6.11633621e-02  8.47723782e-02\n",
            " -5.83404116e-03  2.84162583e-03  2.59479079e-02  3.90261016e-03\n",
            " -5.55586144e-02  5.68111017e-02 -8.95012915e-03 -4.50471742e-03\n",
            " -6.06310219e-02  3.18566561e-02 -6.86047673e-02 -9.39451605e-02\n",
            " -4.23613675e-02  3.93056758e-02 -9.02280435e-02 -5.53663708e-02\n",
            " -1.92777591e-03  5.99909946e-02  9.42121912e-03  6.07980080e-02\n",
            " -5.22598810e-03  1.28727769e-02 -2.15456802e-02 -4.93354201e-02\n",
            " -6.26406254e-05 -4.05308828e-02  7.90260956e-02 -5.32253552e-03\n",
            " -2.70440103e-03 -1.01282876e-02 -6.41689077e-02  2.35941112e-02\n",
            "  4.94743027e-02  1.76912714e-02 -7.69228023e-03  2.03443561e-02\n",
            "  1.55387642e-02  4.64376137e-02  3.61865610e-02  4.38465513e-02\n",
            "  6.95741251e-02  2.37604640e-02 -1.20333163e-03 -4.55584005e-02\n",
            " -3.44719514e-02  1.63743664e-02 -2.70779571e-03  5.60244247e-02\n",
            "  4.04605555e-04 -1.96648743e-02 -3.91405225e-02 -9.61555261e-03\n",
            "  2.14325567e-03  5.61527051e-02  2.82766800e-02  1.80720557e-02\n",
            "  1.23768335e-03 -3.25148106e-02  4.19939868e-02  6.37653470e-02\n",
            "  5.56223281e-02  1.44277038e-02  4.99182940e-02 -1.93575602e-02\n",
            "  5.64282686e-02 -6.90750107e-02  1.60085864e-03  2.10034382e-02\n",
            "  3.52526866e-02  6.35948032e-02 -7.65628293e-02  5.87279573e-02\n",
            " -4.10688967e-02  3.31177264e-02 -2.83569773e-03  7.48986285e-03\n",
            " -3.19783837e-02 -6.25580326e-02  4.89425063e-02  3.36479768e-02\n",
            "  2.12714802e-02  4.28446010e-03 -9.05695604e-04 -4.36472381e-03\n",
            " -9.23611373e-02 -3.89573909e-02  3.30829546e-02  9.61314328e-03\n",
            "  2.15857457e-02 -1.47692738e-02  1.38847362e-02  8.47374573e-02\n",
            " -8.47435743e-02 -4.44485135e-02  7.54610896e-02  1.65129639e-02\n",
            "  2.31740028e-02 -2.84717027e-02 -6.77682534e-02  4.24248539e-02\n",
            " -1.49503639e-02  8.38018022e-03  5.18886261e-02  3.92066427e-02\n",
            " -4.66535538e-02 -2.45939568e-02  7.14570459e-04 -3.46494955e-03\n",
            "  2.36652531e-02  3.31293717e-02 -5.71687194e-03  4.09859084e-02\n",
            " -8.35536122e-02 -1.66684203e-02 -3.52722546e-03  1.73547920e-02\n",
            "  2.53550429e-02 -2.66782343e-02 -2.51309760e-02 -4.68712226e-02\n",
            "  3.96435633e-02 -6.68356707e-03 -3.46517414e-02 -2.81814747e-02\n",
            "  4.19962592e-02 -3.24101560e-02  2.40438618e-02 -5.47125330e-03\n",
            "  2.89679859e-02  6.99753016e-02  3.75228934e-02 -8.82184058e-02\n",
            "  6.07019104e-03 -6.91123381e-02 -1.74329411e-02  2.53294185e-02\n",
            " -7.28369653e-02  2.37329248e-02  4.66296040e-02  4.23278362e-02\n",
            "  5.21276854e-02 -3.55561008e-03 -1.40108112e-02  2.05670819e-02\n",
            "  7.19132945e-02 -7.66954198e-02 -3.90844978e-02 -6.40899539e-02\n",
            "  2.43546739e-02  7.42070824e-02  2.84557994e-02 -7.93435350e-02\n",
            " -1.47222299e-02  6.70734374e-03  1.02058453e-02  1.25844004e-02\n",
            "  3.10074463e-02 -4.17215005e-02  6.03259280e-02  6.22179657e-02\n",
            "  2.10032854e-02 -1.65604409e-02  4.09301184e-03 -3.35808508e-02\n",
            "  7.28031471e-02  6.09729216e-02  4.15963680e-03  8.54913294e-02\n",
            "  1.31301386e-02 -6.34360593e-03 -5.07236086e-02  7.03543425e-02\n",
            "  3.73295508e-02  3.42008099e-02 -3.79101220e-05 -6.27389550e-02\n",
            "  4.80309166e-02 -2.81623453e-02 -7.19844848e-02  1.20989718e-02\n",
            " -4.26681973e-02  1.97790973e-02 -1.35227693e-02 -5.43040968e-02\n",
            " -4.83289734e-02 -2.65408792e-02  8.23441967e-02  5.54602556e-02\n",
            "  7.81148598e-02 -4.00112383e-03  2.02439595e-02  1.84899904e-02\n",
            " -1.52465552e-02  2.18962617e-02  3.17285061e-02  7.06312209e-02\n",
            "  3.70562938e-03  2.40870584e-02  1.94732298e-03 -7.16169551e-03\n",
            " -2.68035308e-02  3.17232199e-02  8.41562822e-02  2.63580363e-02\n",
            " -2.52104383e-02 -1.10670505e-02 -5.61276972e-02  3.97370607e-02\n",
            " -5.51151559e-02  6.12288788e-02  5.18368185e-02 -1.64934061e-02\n",
            "  1.84759719e-03  6.83429018e-02  6.94626421e-02  3.78927328e-02\n",
            " -5.02522849e-02 -2.57219560e-02  7.02316985e-02  3.36491764e-02\n",
            "  5.98843908e-04 -3.67769860e-02  4.44350578e-02  1.09023489e-02\n",
            "  4.34661806e-02 -6.98098615e-02 -2.62659937e-02  2.23396681e-02\n",
            "  9.15533490e-03 -6.15917817e-02  8.41568410e-02  3.92306149e-02\n",
            "  8.92866924e-02  1.28770871e-02  1.39245335e-02 -5.68395145e-02\n",
            " -7.46639073e-02  3.65591794e-02  1.06103569e-02 -6.25316799e-02\n",
            "  1.77653804e-02 -6.67557791e-02 -6.54299483e-02  4.47695665e-02\n",
            " -2.96407975e-02 -2.13827938e-02  3.60825099e-02 -8.18173289e-02\n",
            " -1.40964659e-02  1.33901071e-02  3.07124238e-02 -9.41405520e-02\n",
            "  3.03545557e-02 -2.68576015e-02  1.81658212e-02 -3.67770456e-02\n",
            "  3.33085321e-02  9.13278088e-02 -1.94911137e-02 -1.07955048e-02\n",
            " -9.28005297e-03 -4.06911317e-03  1.46724582e-02 -4.92432937e-02\n",
            "  1.71533599e-02 -5.45221604e-02 -6.61364123e-02 -7.90792033e-02\n",
            "  2.79751848e-02  3.62354182e-02  4.38262261e-02  1.96567848e-02\n",
            " -6.87819719e-02  2.44918838e-02  7.10261464e-02  9.12545249e-02\n",
            "  6.29523173e-02  1.67941395e-02  6.54826015e-02 -4.73535480e-03\n",
            " -4.48028259e-02 -1.61742717e-02  2.20355503e-02 -4.16113250e-02\n",
            "  1.13562578e-02 -3.63931730e-02 -2.76284125e-02  2.01357082e-02\n",
            " -8.22386295e-02 -2.58972924e-02 -4.15527485e-02 -5.79356104e-02\n",
            "  7.72935897e-03  4.06588875e-02 -1.13817379e-02  4.38650511e-02\n",
            " -1.07141351e-02 -4.62955013e-02  5.65050580e-02 -6.18267581e-02\n",
            " -3.84526514e-02 -4.90092300e-02  5.23114428e-02 -5.73057532e-02\n",
            " -1.15798069e-02  5.35165612e-03  2.78308783e-02 -4.59580906e-02\n",
            "  4.34778668e-02 -1.71735901e-02 -1.64880324e-02 -3.88848744e-02\n",
            " -6.44626282e-03  8.33479688e-02  3.65923531e-02  2.13971846e-02\n",
            " -2.59777717e-02 -6.10677898e-02  5.17178811e-02 -5.09534888e-02\n",
            " -8.41620713e-02 -2.87301522e-02 -8.05718377e-02  7.76726380e-02\n",
            "  4.12431993e-02 -3.67930308e-02  2.71156337e-02 -6.33788630e-02\n",
            "  2.25471333e-02 -3.28405760e-02 -9.11881123e-03  5.93739860e-02\n",
            " -7.64809921e-03  8.31474736e-03 -6.06926996e-03  6.88906759e-02\n",
            "  5.30840755e-02 -3.67045030e-03 -2.88561042e-02 -4.73021902e-02\n",
            " -9.18376073e-02 -8.83618277e-03  7.96945244e-02  5.93198873e-02\n",
            "  3.53019126e-02  7.12427963e-03 -7.07041398e-02  1.53526133e-02\n",
            "  4.09509540e-02  7.17417430e-03  7.37813115e-02  3.77231091e-02\n",
            " -9.00578871e-02 -3.24093476e-02  1.16494400e-02 -3.81786190e-02\n",
            "  2.34411880e-02 -2.10632607e-02 -7.03862309e-02  1.52054429e-03\n",
            " -1.61061902e-02  6.18526712e-02  7.94101879e-02  5.88107808e-03\n",
            " -1.51091265e-02 -3.21430154e-02  7.52918655e-03 -1.46267074e-03\n",
            "  6.74486533e-02  8.21412131e-02  1.35811316e-02  1.83010716e-02\n",
            "  3.14529203e-02  3.24921198e-02 -1.57691799e-02 -4.33157757e-02\n",
            " -4.35551889e-02 -1.21416962e-02  3.53560923e-03  4.30949815e-02\n",
            " -2.73922738e-02  2.91704386e-02 -8.02248865e-02 -2.86437925e-02\n",
            " -1.37747321e-02 -3.15168649e-02 -1.11740930e-02 -2.86995899e-02\n",
            " -3.92324254e-02  5.84819168e-02  5.46791963e-03 -2.98926216e-02\n",
            "  5.75806433e-03 -1.14696641e-02 -1.36440666e-02  2.66200826e-02\n",
            "  2.02789833e-03  3.45309786e-02 -5.55947497e-02  4.45481129e-02\n",
            "  7.75338989e-03  3.83887403e-02 -3.04163452e-02 -3.68546396e-02\n",
            " -5.83012551e-02  4.55890112e-02  4.12782058e-02 -2.69910265e-02\n",
            "  6.76821470e-02 -4.02726382e-02 -1.81559287e-02 -3.41260359e-02\n",
            "  5.10256812e-02  1.71382017e-02 -4.04663477e-03 -5.73980510e-02\n",
            "  1.24875875e-02 -6.49295226e-02 -7.56197646e-02  1.49736758e-02\n",
            "  2.50920095e-02  6.72384948e-02  5.28013073e-02  9.06193629e-02\n",
            " -5.04058525e-02  4.25336212e-02  2.04153005e-02 -6.48177508e-03\n",
            "  2.96379179e-02  4.65666084e-03 -4.00494002e-02 -5.69047444e-02\n",
            " -1.83908269e-02  2.06823051e-02  3.54514085e-02 -9.10554156e-02\n",
            " -2.56894790e-02  8.05327892e-02 -6.87960237e-02 -1.90876797e-02\n",
            " -4.92995023e-04 -1.21406829e-02 -8.38927459e-03  8.40041507e-03\n",
            "  7.14359358e-02 -1.60705671e-02  5.72944395e-02  3.03892116e-03\n",
            " -6.78497627e-02 -6.15831316e-02  7.88365304e-02 -6.56958222e-02\n",
            "  4.85723317e-02  6.67115971e-02  8.34890753e-02 -5.98137565e-02\n",
            " -1.69116613e-02  8.24893564e-02 -6.09520636e-03 -3.53485122e-02\n",
            " -2.27540918e-02 -9.13803279e-02 -4.53933701e-02 -8.25901609e-03\n",
            "  4.79642525e-02  7.68054426e-02 -3.04625053e-02 -1.50704673e-02\n",
            " -2.30549723e-02 -8.25553108e-03 -6.91695418e-03 -4.25843114e-04\n",
            "  3.37524414e-02 -2.74846219e-02 -9.42037441e-03 -6.28642179e-03\n",
            " -2.22888868e-02 -1.39351822e-02  6.23975182e-03 -8.72642696e-02\n",
            "  1.45548712e-02 -8.02355036e-02 -3.33043113e-02 -5.46392463e-02\n",
            "  4.86492999e-02  2.14732923e-02  3.27529870e-02 -1.53550990e-02\n",
            "  5.31828627e-02  1.56334788e-02  3.91776711e-02  6.98064500e-03\n",
            " -3.86222564e-02 -2.99131311e-02  1.69515181e-02  5.10435551e-02\n",
            "  3.60743441e-02 -5.03288209e-02  5.67543395e-02  7.83906430e-02\n",
            "  6.19171113e-02 -1.52612077e-02 -1.87837277e-02 -3.86123806e-02\n",
            "  6.28505573e-02 -8.64175484e-02  2.50503188e-04  5.48274927e-02]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "              iii) ELMO\n"
      ],
      "metadata": {
        "id": "I5RnpvvdkT3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the ELMO Model\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_eager_execution()\n",
        "#1024 sized vectors\n",
        "elmo=hub.Module(\"https://tfhub.dev/google/elmo/3\",trainable=True)\n",
        "embeddings=elmo(\n",
        "    sentences,\n",
        "    signature=\"default\",\n",
        "    as_dict=True)[\"elmo\"]\n",
        "init=tf.initialize_all_variables()\n",
        "sess=tf.Session()\n",
        "sess.run(init)\n",
        "print(\"\\n\\n\")\n",
        "print(sess.run(embeddings[0]))\n",
        "print(\"shape=\",embeddings[0].shape)\n"
      ],
      "metadata": {
        "id": "YlYPf5CRcaVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "JsT0vLnvc2nR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "              iv) GP2\n"
      ],
      "metadata": {
        "id": "rmFqADQlkK8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the GP2 Model\n",
        "\n",
        "import transformers\n",
        "gptokenizer=transformers.GPT2Tokenizer.from_pretrained('gpt2-large')\n",
        "model=transformers.GPT2LMHeadModel.from_pretrained('gpt2-large')\n",
        "res_vectors=gptokenizer.encode(para,add_special_tokens=False,return_tensors=\"pt\")\n",
        "print(\"shape=\",res_vectors.shape)\n",
        "res_vectors"
      ],
      "metadata": {
        "id": "eeFleD3Ycsa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "                v) Sentence-BERT"
      ],
      "metadata": {
        "id": "9v3xAZOzjtwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing sentencebert model\n",
        "\n",
        "bert = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim128/2\")\n",
        "embeddings=bert(sentences)\n",
        "print(embeddings)"
      ],
      "metadata": {
        "id": "V9WDAXsJddyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"shape=\",embeddings[0].shape)\n",
        "#each sentence is converted into vector having 128 values\n",
        "print(\"The sentence in the paragraph: \",sentences[0],\"\\n is converted into vector  as : \\n{}\".format(embeddings[0]))\n",
        "shape= (128,)"
      ],
      "metadata": {
        "id": "zIu2PfEFd0yI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "              2.Find named entities (NER) for the above paragraph?\n"
      ],
      "metadata": {
        "id": "UOoNEWjilBBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "ner=spacy.load('en_core_web_sm')\n",
        "res=ner(para)\n",
        "\n",
        "for word in res.ents:\n",
        "  print(word.text,word.label_)"
      ],
      "metadata": {
        "id": "WxkwFc0Vd3o4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.explain('GPE')"
      ],
      "metadata": {
        "id": "YSlWDM9aeGTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#better visualisation of entity recognition\n",
        "displacy.render(res,style=\"ent\",jupyter=True)"
      ],
      "metadata": {
        "id": "sRK8qsl4eK27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "vcMNWbpZeeHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "           3.Find similar sentences (repeated sentences) from the above paragraph? (Cosine Similarity, use BERT to encode)\n"
      ],
      "metadata": {
        "id": "LH4w0mrLlMwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')"
      ],
      "metadata": {
        "id": "riSRPkDAePp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using BERT to Encode\n",
        "se_embeddings = sbert_model.encode(sentences)\n",
        "q1_vec= sbert_model.encode(sentences[0])\n",
        "\n",
        "#cosine similarity function\n",
        "#identifies similarity between 2 sentences\n",
        "def cosine(u, v):\n",
        "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
        "\n",
        "for sent in sentences:\n",
        "  sim = cosine(q1_vec, sbert_model.encode([sent])[0])\n",
        "  #if similarity ==1 => repeated sentence\n",
        "  #if similarity > 0.6 => similar sentence\n",
        "  if sim>0.6:\n",
        "    print(\"Sentence1 =\",sentences[0],\"\\n \\nSentence2=\", sent, \"\\n\\nsimilarity = \", sim,end=\"\\n ----------------------------- \\n\")"
      ],
      "metadata": {
        "id": "EquuddrWlwph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Explain POS tagging with HMM? Tag POS for the above paragraph?\n",
        "POS Tagging with HMM\n",
        "\n",
        "Parts of Speech Tagging (POS): It is a process of converting a sentence to forms – list of words, list of tuples (where each tuple is having a form (word, tag)). The tag in case of is a part-of-speech tag, and signifies whether the word is a noun, adjective, verb, and so on. reading a sentence and being able to identify what words act as nouns, pronouns, verbs, adverbs, and so on. All these are referred to as the part of speech tags. According to Wikipedia, part-of-speech tagging (POS tagging or PoS tagging or POST), also called grammatical tagging or word-category disambiguation, is the process of marking up a word in a text as corresponding to a particular part of speech, based on both its definition and its context i.e. its relationship with adjacent and related words in a phrase, sentence, or paragraph.\n",
        "\n",
        "Getting Started\n",
        "\n",
        "Default tagging is a basic step for the part-of-speech tagging. It is performed using the DefaultTagger class. The DefaultTagger class takes ‘tag’ as a single argument. NN is the tag for a singular noun. DefaultTagger is most useful when it gets to work with most common part-of-speech tag. that’s why a noun tag is recommended.\n",
        "\n",
        "Getting Started\n",
        "\n",
        "POS tagging with Hidden Markov Model HMM (Hidden Markov Model) is a Stochastic technique for POS tagging. Hidden Markov models are known for their applications to reinforcement learning and temporal pattern recognition such as speech, handwriting, gesture recognition, musical score following, partial discharges, and bioinformatics. Let us consider an example proposed by Dr.Luis Serrano and find out how HMM selects an appropriate tag sequence for a sentence.\n",
        "\n",
        "Getting Started\n",
        "\n",
        "In this example, we consider only 3 POS tags that are noun, model and verb. Let the sentence “ Ted will spot Will ” be tagged as noun, model, verb and a noun and to calculate the probability associated with this particular sequence of tags we require their Transition probability and Emission probability.\n",
        "\n",
        "The transition probability is the likelihood of a particular sequence for example, how likely is that a noun is followed by a model and a model by a verb and a verb by a noun. This probability is known as Transition probability. It should be high for a particular sequence to be correct.\n",
        "\n",
        "Now, what is the probability that the word Ted is a noun, will is a model, spot is a verb and Will is a noun. These sets of probabilities are Emission probabilities and should be high for our tagging to be likely.\n",
        "\n",
        "Let us calculate the above two probabilities for the set of sentences below\n",
        "\n",
        "Mary Jane can see Will Spot will see Mary Will Jane spot Mary? Mary will pat Spot Note that Mary Jane, Spot, and Will are all names.\n",
        "\n",
        "In the above figure, we can see that the < S > tag is followed by the N tag three times, thus the first entry is 3.The model tag follows the < S > just once, thus the second entry is 1. In a similar manner, the rest of the table is filled.\n",
        "\n",
        "Next, we divide each term in a row of the table by the total number of co-occurrences of the tag in consideration, for example, The Model tag is followed by any other tags four times (in total) as shown below, thus we divide each element in the third row by four.\n",
        "\n",
        "the table is refined as below:\n",
        "\n",
        "Getting Started\n",
        "\n",
        "Calculating the product of these terms we get,\n",
        "\n",
        "3/41/93/91/43/41/414/94/9=0.00025720164\n",
        "\n",
        "For our example, keeping into consideration just three POS tags we have mentioned, 81 different combinations of tags can be formed. In this case, calculating the probabilities of all 81 combinations seems achievable. But when the task is to tag a larger sentence and all the POS tags in the Penn Treebank project are taken into consideration, the number of possible combinations grows exponentially and this task seems impossible to achieve. Now let us visualize these 81 combinations as paths and using the transition and emission probability mark each vertex and edge as shown below.\n",
        "\n",
        "Getting Started\n",
        "\n",
        "Now there are only two paths that lead to the end, let us calculate the probability associated with each path.\n",
        "\n",
        "s→N→M→N→N→ E =3/41/93/91/41/42/91/94/94/9=0.00000846754\n",
        "\n",
        "s→N→M→N→V→E=3/41/93/91/43/41/414/94/9=0.00025720164\n",
        "\n",
        "Clearly, the probability of the second sequence is much higher and hence the HMM is going to tag each word in the sentence according to this sequence.\n",
        "\n"
      ],
      "metadata": {
        "id": "3XCyCdC7jRRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "7U6iDXS4r8p5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Tagging for Above Given Paragraph\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "stop_words = set(stopwords.words('english'))\n",
        " \n",
        "txt = \"A paragraph is a series of sentences that are organized and coherent, and are all related to a single topic.\" \\\n",
        "    \"Almost every piece of writing you do that is longer than a few sentences should be organized into paragraphs.\" \\\n",
        "    \"This is because paragraphs show a reader where the subdivisions of an essay begin and end, and thus help the reader see the organization of the essay and grasp its main points.\" \\\n",
        "    \"Paragraphs can contain many different kinds of information.\" \\\n",
        "    \"A paragraph could contain a series of brief examples or a single long illustration of a general point.\" \\\n",
        "    \"It might describe a place, character, or process; narrate a series of events; compare or contrast two or more things; classify items into categories; or describe causes and effects.\" \\\n",
        "    \"Regardless of the kind of information they contain, all paragraphs share certain characteristics.\" \\\n",
        "    \"One of the most important of these is a topic sentence.\"\n",
        " \n",
        "# sent_tokenize is one of instances of\n",
        "# PunktSentenceTokenizer from the nltk.tokenize.punkt module\n",
        " \n",
        "tokenized = sent_tokenize(txt)\n",
        "for i in tokenized:\n",
        "     \n",
        "    # Word tokenizers is used to find the words\n",
        "    # and punctuation in a string\n",
        "    wordsList = nltk.word_tokenize(i)\n",
        " \n",
        "    # removing stop words from wordList\n",
        "    wordsList = [w for w in wordsList if not w in stop_words]\n",
        " \n",
        "    #  Using a Tagger. Which is part-of-speech\n",
        "    # tagger or POS-tagger.\n",
        "    tagged = nltk.pos_tag(wordsList)\n",
        " \n",
        "    print(tagged)"
      ],
      "metadata": {
        "id": "cyPQikEbllcZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}